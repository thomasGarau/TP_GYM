{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        ...,\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [235, 245, 249],\n",
       "        [204, 230, 255],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [235, 245, 249],\n",
       "        [235, 245, 249],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [235, 245, 249],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [235, 245, 249],\n",
       "        [235, 245, 249],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        ...,\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230]]], dtype=uint8)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v1',render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "env.render() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.00011021673300177029,\n",
       " 1: 0.0016638070645010985,\n",
       " 2: 0.00038090769822518903,\n",
       " 3: 3.2403933989993984e-06,\n",
       " 4: 0.0008436071611495153,\n",
       " 5: 0,\n",
       " 6: 2.592437387262579e-05,\n",
       " 7: 0,\n",
       " 8: 0.00540309860823849,\n",
       " 9: 0.04701349524845494,\n",
       " 10: 0.0015042226732940624,\n",
       " 11: 0,\n",
       " 12: 0,\n",
       " 13: 0.5542653223278835,\n",
       " 14: 0.15784616563943443,\n",
       " 15: 0}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_value = {state: 0 for state in range(env.observation_space.n)}\n",
    "gamma = 0.90\n",
    "alpha = 0.85\n",
    "num_episodes = 50000\n",
    "num_timesteps = 1000\n",
    "def random_policy(action_space):\n",
    "    return action_space.sample()\n",
    "\n",
    "def value_iteration(state_value, env, alpha, gamma, num_episodes, num_iteration, ):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        for t in range(num_timesteps):\n",
    "            action = random_policy(env.action_space)\n",
    "            next_state, reward, done, info, prob = env.step(action)\n",
    "            state_value[state] += alpha * (reward + gamma * state_value[next_state] - state_value[state])\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "value_iteration(state_value, env, alpha, gamma, num_episodes, num_timesteps)\n",
    "state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 1: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 2: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 3: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 4: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 5: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 6: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 7: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 8: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 9: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 10: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 11: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 12: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 13: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 14: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}, 15: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}}\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.2\n",
    "gamma = 0.90 \n",
    "epsilon = 0.4\n",
    "num_episodes = 50000\n",
    "num_timesteps = 1000 \n",
    "def initialize_Q():\n",
    "    Q = {}\n",
    "    for state in range(env.observation_space.n):\n",
    "        Q[state] = {action: 0.0 for action in range(env.action_space.n)}\n",
    "    return Q\n",
    "Q = initialize_Q()\n",
    "print(Q)\n",
    "\n",
    "def epsilon_greedy(state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        max = float('-inf')\n",
    "        max_action = 0\n",
    "        for action in Q[state]:\n",
    "            if Q[state][action] > max:\n",
    "                max = Q[state][action]\n",
    "                max_action = action\n",
    "        return max_action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2, 2.3 ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: 0.015494614492085902,\n",
       "  1: 0.012033979281374025,\n",
       "  2: 0.01187231124153694,\n",
       "  3: 0.010641823275048099},\n",
       " 1: {0: 0.008212555172715262,\n",
       "  1: 0.0025879490968668067,\n",
       "  2: 0.0060931009629624205,\n",
       "  3: 0.014462630981095292},\n",
       " 2: {0: 0.012732084685666873,\n",
       "  1: 0.020995100653165157,\n",
       "  2: 0.009509007817324278,\n",
       "  3: 0.011669192939537155},\n",
       " 3: {0: 0.006406739852567465,\n",
       "  1: 0.00657520506575367,\n",
       "  2: 0.008482801178621758,\n",
       "  3: 0.010500412156802289},\n",
       " 4: {0: 0.030740005612855108,\n",
       "  1: 0.02003614008369154,\n",
       "  2: 0.014835757170528005,\n",
       "  3: 0.007880890507935627},\n",
       " 5: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
       " 6: {0: 0.012614535619503191,\n",
       "  1: 0.012995382265449018,\n",
       "  2: 0.04827850825865515,\n",
       "  3: 0.0034568636735987854},\n",
       " 7: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
       " 8: {0: 0.01686502423077649,\n",
       "  1: 0.03313277307402587,\n",
       "  2: 0.08026068126069535,\n",
       "  3: 0.05565145582806635},\n",
       " 9: {0: 0.029150124757949775,\n",
       "  1: 0.1695602923914873,\n",
       "  2: 0.12224586931980716,\n",
       "  3: 0.06955397377894215},\n",
       " 10: {0: 0.17915588086566406,\n",
       "  1: 0.17480310992425158,\n",
       "  2: 0.1763706732273309,\n",
       "  3: 0.02594121887304061},\n",
       " 11: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
       " 12: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
       " 13: {0: 0.05279223867481244,\n",
       "  1: 0.2291760254199248,\n",
       "  2: 0.2528546200986525,\n",
       "  3: 0.1191792104430878},\n",
       " 14: {0: 0.18682157932994878,\n",
       "  1: 0.6702739337615772,\n",
       "  2: 0.41014447457062775,\n",
       "  3: 0.38875161662444796},\n",
       " 15: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sarsa(Q, env, alpha, gamma, epsilon, num_episodes, num_timesteps):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        action = epsilon_greedy(state, epsilon)\n",
    "\n",
    "        for t in range(num_timesteps):\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state_key = next_state\n",
    "            next_action = epsilon_greedy(next_state_key, epsilon)\n",
    "\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state_key][next_action] - Q[state][action])\n",
    "            state, action = next_state_key, next_action\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "sarsa(Q, env, alpha, gamma, epsilon, num_episodes, num_timesteps)\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 3.2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.85\n",
    "gamma = 0.90 \n",
    "epsilon = 0.8\n",
    "num_episodes = 50000\n",
    "num_timesteps = 1000 \n",
    "def initialize_Q():\n",
    "    Q = {}\n",
    "    for state in range(env.observation_space.n):\n",
    "        Q[state] = {action: 0.0 for action in range(env.action_space.n)}\n",
    "    return Q\n",
    "Q = initialize_Q()\n",
    "\n",
    "def epsilon_greedy(state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        max = float('-inf')\n",
    "        max_action = 0\n",
    "        for action in Q[state]:\n",
    "            if Q[state][action] > max:\n",
    "                max = Q[state][action]\n",
    "                max_action = action\n",
    "        return max_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: 0.3316293390447249,\n",
       "  1: 0.3818852936925448,\n",
       "  2: 0.38886503769242764,\n",
       "  3: 0.3881841540821364},\n",
       " 1: {0: 0.3278808691891414,\n",
       "  1: 0.36496127257762856,\n",
       "  2: 0.49323958980812954,\n",
       "  3: 0.3793040666477189},\n",
       " 2: {0: 0.5900067572815975,\n",
       "  1: 0.481663029484368,\n",
       "  2: 0.5846632425197472,\n",
       "  3: 0.5206210271243369},\n",
       " 3: {0: 0.5198578124175512,\n",
       "  1: 0.08041874964530188,\n",
       "  2: 0.010059860646737258,\n",
       "  3: 0.44806789481818166},\n",
       " 4: {0: 0.4233338545354607,\n",
       "  1: 0.3751539959079401,\n",
       "  2: 0.42667280971163335,\n",
       "  3: 0.008294636495374236},\n",
       " 5: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
       " 6: {0: 0.48263931782020475,\n",
       "  1: 0.6690980537836765,\n",
       "  2: 0.6037295108403103,\n",
       "  3: 0.011762648662912004},\n",
       " 7: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
       " 8: {0: 0.32519631183970926,\n",
       "  1: 0.47372111320017163,\n",
       "  2: 0.503877659499414,\n",
       "  3: 0.47777493402050975},\n",
       " 9: {0: 0.5360732075855427,\n",
       "  1: 0.5115550509871994,\n",
       "  2: 0.08458323629222242,\n",
       "  3: 0.06612244168671139},\n",
       " 10: {0: 0.6094912408927059,\n",
       "  1: 0.07146278783407545,\n",
       "  2: 0.8115125787145452,\n",
       "  3: 0.4600626134521338},\n",
       " 11: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
       " 12: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0},\n",
       " 13: {0: 0.08169398850520376,\n",
       "  1: 0.07624510178334554,\n",
       "  2: 0.5691129518495318,\n",
       "  3: 0.1029624755675067},\n",
       " 14: {0: 0.5908133655853774,\n",
       "  1: 0.6344612330354714,\n",
       "  2: 0.9935608702211419,\n",
       "  3: 0.6933584867165928},\n",
       " 15: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def q_learning(Q, env, alpha, gamma, epsilon, num_episodes, num_timesteps):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        for t in range(num_timesteps):\n",
    "            action = epsilon_greedy(state, epsilon)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            best_next_action = max(Q[next_state], key=Q[next_state].get)\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state][best_next_action] - Q[state][action])\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "q_learning(Q, env, alpha, gamma, epsilon, num_episodes, num_timesteps)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5360732075855427,\n",
       " 1: 0.5115550509871994,\n",
       " 2: 0.08458323629222242,\n",
       " 3: 0.06612244168671139}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[9]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
