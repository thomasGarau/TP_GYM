{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        ...,\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [235, 245, 249],\n",
       "        [204, 230, 255],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [235, 245, 249],\n",
       "        [235, 245, 249],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [235, 245, 249],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [235, 245, 249],\n",
       "        [235, 245, 249],\n",
       "        ...,\n",
       "        [204, 230, 255],\n",
       "        [204, 230, 255],\n",
       "        [180, 200, 230]],\n",
       "\n",
       "       [[180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        ...,\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230],\n",
       "        [180, 200, 230]]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1',render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 4, 0.0, False)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0.8235294117646954, 0],\n",
       " [1, 0.8235294117646924, 3],\n",
       " [2, 0.8235294117646904, 3],\n",
       " [3, 0.8235294117646894, 3],\n",
       " [4, 0.8235294117646963, 0],\n",
       " [5, 0.0, 0],\n",
       " [6, 0.5294117647058754, 0],\n",
       " [7, 0.0, 0],\n",
       " [8, 0.8235294117646978, 3],\n",
       " [9, 0.8235294117646998, 1],\n",
       " [10, 0.764705882352936, 0],\n",
       " [11, 0.0, 0],\n",
       " [12, 0.0, 0],\n",
       " [13, 0.8823529411764661, 2],\n",
       " [14, 0.9411764705882328, 1],\n",
       " [15, 0.0, 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 1\n",
    "tab_value = [[i, 0, -1] for i in range(env.observation_space.n)]\n",
    "nb_iteration = 1000\n",
    "treshold = 1e-20\n",
    "\n",
    "def valeur_optimal() :\n",
    "    i = 0  \n",
    "    valueFind = False\n",
    "    \n",
    "    while i < nb_iteration and valueFind != True:\n",
    "        a = calcule_tab()\n",
    "        iteration()\n",
    "        b = calcule_tab()\n",
    "        if(abs(a - b) < treshold): valueFind = True\n",
    "        i +=1      \n",
    "\n",
    "def calcule_tab(): \n",
    "    a = 0\n",
    "    for q in tab_value : \n",
    "        a += q[1] \n",
    "    return a\n",
    "\n",
    "def iteration():\n",
    "    for i in range(env.observation_space.n):\n",
    "        tab_value[i][1], tab_value[i][2] = state_value(i)\n",
    "\n",
    "def state_value(state):\n",
    "    a = -1\n",
    "    best_action = -1\n",
    "    for i in range(env.action_space.n):\n",
    "        b = value_q(state,i)\n",
    "        if(b>a):\n",
    "            a = b \n",
    "            best_action = i\n",
    "    return a, best_action\n",
    "\n",
    "def value_q(state, action):\n",
    "    b = 0\n",
    "    a = env.P[state][action]\n",
    "    for i in range(len(a)): \n",
    "        b += a[i][0] * (a[i][2] + gamma * tab_value[a[i][1]][1])\n",
    "    return b\n",
    "\n",
    "valeur_optimal()\n",
    "tab_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 4, 0.0, False),\n",
       " (0.3333333333333333, 1, 0.0, False),\n",
       " (0.3333333333333333, 0, 0.0, False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: [0, 3, 3, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "gamma = 1\n",
    "threshold = 1e-20\n",
    "max_iterations = 1000\n",
    "policy = [random.randint(0, env.action_space.n - 1) for x in range(env.observation_space.n)]\n",
    "tab_value = [[i, 0] for i in range(env.observation_space.n)]\n",
    "\n",
    "def compute_value_function(policy):\n",
    "    for i in range(max_iterations):\n",
    "        prev_tab_value = [value for x, value in tab_value]\n",
    "        for state in range(env.observation_space.n):\n",
    "            action = policy[state]\n",
    "            tab_value[state][1] = sum([prob * (reward + gamma * prev_tab_value[next_state])\n",
    "                                       for prob, next_state, reward, done in env.P[state][action]])\n",
    "        if all(abs(tab_value[state][1] - prev_tab_value[state]) < threshold for state in range(env.observation_space.n)):\n",
    "            break\n",
    "\n",
    "def extract_policy():\n",
    "    for state in range(env.observation_space.n):\n",
    "        Q_values = []\n",
    "        for action in range(env.action_space.n):\n",
    "            Q_value = sum([prob * (reward + gamma * tab_value[next_state][1])\n",
    "                           for prob, next_state, reward, done in env.P[state][action]])\n",
    "            Q_values.append(Q_value)\n",
    "        policy[state] = Q_values.index(max(Q_values))\n",
    "\n",
    "def policy_iteration():\n",
    "    for _ in range(max_iterations):\n",
    "        prev_policy = policy.copy()\n",
    "        compute_value_function(policy)\n",
    "        extract_policy()\n",
    "        if policy == prev_policy:\n",
    "            break\n",
    "\n",
    "policy_iteration()\n",
    "print(\"Optimal Policy:\", policy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
